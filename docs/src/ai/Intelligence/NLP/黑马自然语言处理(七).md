# 1、注意力机制

观察事物时，能够快速判断一种事物，是因为大脑**能把注意力放在事物最具有辨识度的地方**，从而可快速判断。并非是从头到尾的观察一遍事物后，才能有判断结果。正是基于这样的理论，就产生了注意力机制。



人类视觉的注意力机制：

- **人类视觉**通过长期进化形成**视觉注意力机制**，是**大脑信号处理机制**
- 会快速扫描全局图像，获得需要重点关注的目标区域
- 比如：更多投入到**人的脸部，文本的标题**以及**文章首句等位置**

> 抛砖：能不能把这种**注意力机制**引入到**深度神经网络**中？

注意力机制是一个非常重要、强大的工具：

- 是一种在深度学习中常用的技术，它允许模型在**处理序列数据**时能够**集中关注输入数据**的不同部分
- **自然语言处理中**，注意力机制被广泛应用于机器翻译（比如Transformer模型）、文本摘要、问答系统等任务。
- **图像处理中**，比如在图像描述生成、图像分类等任务中，甚至扩展到了多模态任务，如图像与文本之间的关联问题。
- 注意力机制是一个非常重要且强大的工具，可以帮助模型更好地理解和利用**输入数据的信息**。

> 抛砖：与RNN**提取事物特征**对比，说一说为什么要引入到注意力机制？

从RNN模型角度来看：

- rnn一个时间步一个时间步的提取事物特征，**效率低**
- rnn提取长序列特征时，前面单词的特征会遗忘，**没有重点**

从带有注意力机制的模型来看：

- 注意力机制可以**并行**提取事物特征，而且还能注意到**关键点**
- **带有注意力机制的模型，能够更加灵活的提取事物特征**

> 抛砖：如何把我们的数据，也就是**特征张量数据输入**到注意力机制中呢？

## 1.1、注意力机制的输入QKV

Query（查询）、Key（键）和Value（值）

- Query (Q)**查询向量**:表示输入序列要查询的问题
- Key (K)**键向量**:表示关注的内容的索引（关键字）
- Value (V)**值向量**:表示关注的内容
- QKV是注意力机制中的核心组件，它们协同工作以使模型能够灵活地关注输入数据的不同部分，从而提升模型的性能和表达能力

例如：在档案柜中找某个文件：

- Q(query) 查询张量，**相当于正在研究的课题**；比如：写在便利贴上
- K（key）索引张量，**相当于文件夹上贴的标签**；比如：key#1 key#2
- V（value）内容张量，**相当于正在查找的内容**；比如：文档、书籍资料
- 在这个栗子中，**Q和K进行比较相关性（相识度），然后再从V中找到关心的内容**

例如：自然语言处理问答任务中：

- 查询（Q）：这个新闻的主题是关于什么的？
- 键（K）：这个新闻报道了当地政府的新政策。
- 值（V）：这个新闻说了政府的新政策是什么。
- 在这个例子中，查询是询问新闻的主题。模型通过比较查询和键（这里是两个新闻报道）**来确定哪个报道更符合查询的主题。然后，值提供了报道中的具体信息**，以帮助模型更好地理解主题。

例如：文本生成任务中：

- 查询（Q）：请给我生成一段关于环保的短文。
- 键（K）：环保。
- 值（V）：我们应该采取一系列措施来保护环境。
- 在这个例子中，查询是关于生成一段关于环保的短文。通过比较查询和键，**模型可以确定需要生成的内容应该与环保相关**。然后，**值提供了一个关于环保的具体建议**，以帮助模型生成合适的文本。





## 1.2、注意力机制的实现步骤

1. 用查询张量q 与key 进行运算（**相似度运算**）得到一个注意力机制的权重分布（**就是一个权重系数**）
2. 用权重分布乘以内容得到**注意机制的结果表示**

例如：我们给模型一句话：

```bash
# 机器人必须服从人类给它的命令
A robot must obey the orders given it by human beings
```

模型看到上面那句话，模型怎么知道其中的 `it` 是代指谁呢？对上面的话进行分析：

- q：要查询的问题就是 it
- k：就是上面话每个单词的权重
- v：就是上面话每个单词的词向量表示

1. 第一步：用查询张量q 与key 进行运算：即用 it 和其余单词进行**相似度运算**，得到一个权重分布。

2. 第二步：用每个单词的词向量表示 v 和第一步得到的权重分布分别相乘并加和。

![](黑马自然语言处理(七).assets/3.png)





例如上图：

1. 首先将  it 和其余单词进行**相似度运算**，得到每个权重分布
2. 用每个权重分布和各自单词的词向量分别相乘再求和，这样就可以将 q 升级为一个更强大的 qplus
3. 背后的意义是：值向量加权混合得到的结果也是一个向量, 它将其50%的注意力放在了单词"robot"上, 30%的注意力放在了"a"上, 还有19%的注意力放在了"it"上
4. 函数意义：attaction(q,v,k) -> attention_weights, qplus
   - attention是关于qkv的函数，通过qkv运算，**得到权重分布，并且普通的q升级成一个更加强大的qplus**

# 2、Seq2Seq架构

Sequence-to-Sequence架构：

- 是一种在自然语言处理领域中非常重要的**模型结构**，广泛应用于诸如机器翻译、文本摘要、对话系统等任务
- 该架构的核心思想是将输入序列映射为一个**中间表示(中间语义张量c)**，然后**再从这个中间表示**生成目标序列

例如下方的中文翻译为英文的案例：我们希望输入 `欢迎来北京` 输出 `welcome to BeiJing `

![](黑马自然语言处理(七).assets/1.png)





- seq2seq模型架构包括三部分，分别是**encoder(编码器)、decoder(解码器)、中间语义张量c**
- 编码流程：1个时间步1个时间步的编码，每个时间步有隐藏层输出，**最终组合成中间语义张量C**
- 解码流程：1个时间步1个时间步的解码，input和h0输出 output和hn
  - 再接一个**全连接层+softmax做一个分类**，从分类结果中找一个预测结果即可

> 其实文本翻译就是一个多分类问题，我们希望`欢迎`对应`welcome`，但是预测值可能和正确答案`welcome`有误差，就进行反向传播修正权重参数，直到匹配到正确的分类。

编码器Encoder：

- 编码器负责将输入序列（例如一段文本）**映射成一个中间的表示（中间语义张量C）**，通常是一个固定长度的向量
- 常用的编码器结构是**循环神经网络（RNN）**或者长短时记忆网络（LSTM），近年来也包括了**Transformer模型**

编码器的工作流程：

- 对输入的文本序列**每个时间步都会产生一个隐藏状态**。这些隐藏状态会捕捉到输入序列的信息，并被传递给下一个时刻
- 在Seq2Seq中，编码器会将整个输入序列处理完后，**最后一个时刻的隐藏状态**会被用作**解码器的初始隐藏状态**

解码器Decoder：

- 解码器接受编码器传递过来的初始隐藏状态，并逐步**生成目标序列**
- 解码器**也是一个RNN系列模型，通常会包含一个额外的注意力机制**，以便在生成目标序列过程中动态地关注输入序列的不同部分

解码器的工作流程：

- 一个时间步一个时间步的解码；采用自回归机制进行解码
- 每个时间步都会用到中间语义张量C



## 2.1、解码器中加入注意力机制

![](黑马自然语言处理(七).assets/2.png)

解码中：一个时间步一个时间步的解码，每个时间步都要添加注意力机制，每个时间步都需要引入qkv

- 正常情况：
  - 输入welcome的查询张量q(也就是上图的GO)，再加上上一个时间步的隐藏层输出S0，送给GUR进行解码，再接**全连接+SoftMax**做分类得到welcome的词向量表示。
  - 然后输入 to 的查询张量q(也就是上图的welcom)，再加上上一个时间步的隐藏层输出S1，送给GUR进行解码，再接**全连接+SoftMax**做分类得到 to 的词向量表示。

> 例如GO的形状是[1,32]，经过RNN网络GRU后变成[1,128]，然后经过全连接层做SoftMax作分类，变成[1,450]。也就是有450个分类
>
> ![](黑马自然语言处理(七).assets/5.png)

- 解码器中加入注意力机制
  - 输入welcome的查询张量q(也就是上图的GO)，再加上中间语义C，再加上上一个时间步的隐藏层输出S0，三个参数进行 `attaction(q,v,k) -> attention_weights, qplus`
    - q的查询目标是谁，就是谁的查询张量
    - k：上一个时间步的隐藏层输出
    - v：中间语义张量C

> [!note]
>
> 在Sqe2Sqe架构中添加注意力机制，产生 to 时权重分布如果不准确怎么办？
>
> - 理解**注意力机制**应该站在整个深度学习网络三大件的高度来理解。注意力机制只不过是前向计算（前向传播）路径中的**一个小路径，一个小的策略**，最终还是要靠损失函数、反向传播来**更新注意力机制层的权重参数**，来学习事物的特征！





## 2.2、注意力机制的常见计算规则

注意力机制有30多种计算规则，我们这边简单看几种常用的即可：

- 计算规则的目的就是**得到权重参数、得到加强后的qplus**

![](黑马自然语言处理(七).assets/4.png)



## 2.3、注意力机制的分类和作用

1. 根据传入的QKV的不同分为2类：
   - **一般注意力机制**：Q与KV不相同，比如：: Q != K != V ，也就是Q、K、V都不相等； 或者 Q != （K == V）也就是K和V相等，但是与Q不相等
   - **自注意机制**：Q == K == V， 也就是Q、K、V都相等
2. 注意力机制的作用
   - **在解码器端的注意力机制**：能够根据模型目标有效的聚焦编码器的输出结果（**比如更加聚焦中间语义张量C**），提升解码器的解码提升效果
   - **在编码器端的注意力机制**：可以并行提取文本序列特征，支持长文本特征提取。增强了特征提取能力，一般使用**自注意力**，也就是上面的第三个注意力计算规则



# 3、Seq2Seq英译法

> 待更新！





































