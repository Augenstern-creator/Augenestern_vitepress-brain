# 1、Transformer

相比之前占领市场的LSTM和GRU模型，Transformer有两个显著的优势：

- Transformer能够利用分布式GPU进行**并行训练**，提升模型训练效率
- 在分析预测更长的文本时, **捕捉间隔较长的语义**关联效果更好

RNN、LSTM、Transformer对长文本**提取事物特征**效果对比：

- rnn和Lstm文本长度**20~30**之间效果下降显著
- transformer**超过句子长度40**以后也能保持较好效果

> 扩展：
>
> - SOTA model：state-of-the-art model，并不是特指某个具体的模型，而是指在该项研究任务中，目前最好最先进的模型
> - NLP是人工智能的一个子域，2个核心任务：**自然语言理解和自然语言生成**(GPT)



## 1.1、Transformer架构

基于seq2seq架构的Transformer模型可以完成NLP领域研究的典型任务，如机器翻译, 文本生成等。同时又可**构建预训练语言模型**，用于不同任务的迁移学习。

假设使用Transformer模型架构，处理从**一种语言文本**到**另一种语言文本**的翻译工作：

![](黑马Transformer(八).assets/1.png)

输入部分：

- **源文本嵌入层**及其**位置编码器**(英文的词向量表示和英文的位置编码)
- **目标文本嵌入层**及其**位置编码器**(法文的词向量表示和法文的位置编码)

![](黑马Transformer(八).assets/2.png)



输出部分：

- 全连接层(线性层)
- Softmax层

![](黑马Transformer(八).assets/3.png)



编码器部分：

- 由**N个编码器层**堆叠而成(上图是1个编码器层)
- 每个编码器层由**两个子层连接**结构组成
  - 第一个子层连接结构包括一个**多头自注意力子层**和**规范化层**以及**一个残差连接**
  - 第二个子层连接结构包括一个**前馈全连接子层**和**规范化层**以及**一个残差连接**

![](黑马Transformer(八).assets/4.png)

输出部分：

- 由**N个解码器层**堆叠而成(图中是1个解码器层)
- 每个解码器层由**三个子层连接**结构组成
  - 第一个子层连接结构包括一个**多头自注意力子层**和**规范化层**以及一个**残差连接**
  - 第二个子层连接结构包括一个**多头注意力子层**和**规范化层**以及一个**残差连接**
  - 第三个子层连接结构包括一个**前馈全连接子层**和**规范化层**以及一个**残差连接**

![](黑马Transformer(八).assets/5.png)





### 1.1.1、文本嵌入层

![](黑马Transformer(八).assets/6.png)

### 1.1.2、位置编码器

为什么要给文本添加位置信息？

- 栗子1： 在教室**中**间，唱我爱**中**国。**中**字在句子的不同位置，表示的含义不同

给文本添加位置信息，准确的说是：给**文本特征**添加**位置特征信息**，通过**位置编码**来实现，**位置编码函数**以当前元素在序列中的位置作为输入，然后使用一组预定义的参数（即位置参数）来生成对应位置的向量，这个过程可以表示为：
$$
PE(pos,2i) = sin(pos / 10000^{2i/d\_model}) 
$$

$$
PE(pos,2i+1) = cos(pos / 10000^{2i/d\_model}) 
$$

- PE是位置编码函数，是关于单词位置pos和奇偶特征的函数，pos代表当前元素的位置，i代表嵌入向量的第几个特征（**奇数特征、偶数特征**），d_model代表嵌入向量的维度。通过这种方式，Transformer可以在每个输入元素的嵌入向量中**加入其位置信息**，从而在训练过程中自动学习到元素间的**相对位置关系**

> - 给数据的偶数特征，添加sin特征值曲线
> - 给数据的奇数特征，添加cos特征值曲线



### 1.1.3、掩码张量-下三角矩阵

以模型解码为例，生成字符时，一个时间步一个时间步的解码：

- 使用**掩码mask**（比如：0表示能看的见， **1表示被遮掩**） 希望模型不要使用当前字符和后面的字符。**也就是防止模型看到未来信息，用1给他遮掩住**）

![](黑马Transformer(八).assets/7.png)



1. 第1个时间步：4个1表示要生成的4个字符全都被遮掩
2. 第2个时间步：只能是看到**我**，也就是第1个时间步的预测结果
3. 第3个时间步：只能是看到**我，爱**，也就是第1、2个时间步的预测结果
4. 第4个时间步：只能是看到**我，爱，中**，也就是第1、2、3个时间步的预测结果

> 因为我们将输入和目标都输入到模型，有时候并不希望模型能直接看到答案，而是让模型给出预测值，这样就可以计算误差。



### 1.1.4、自注意力机制

![](黑马Transformer(八).assets/8.png)



- 从上式可以看出其计算过程为：首先，计算矩阵Q和K每一行向量的内积，**为了防止内积过大，除以d_k的平方根**；其次，**使用Softmax对上述内积的结果进行归一化；**最后得到Softmax矩阵之后和V相乘，得到最终的输出。
- 可以看出QKV的来源都是输入矩阵X(词向量)与矩阵的乘积，本质上都是X的线性变换（代码上就是分别经过了三个线性层），这也是为什叫做自注意力机制的原因。
- QK^T^ 得到的就是权重参数，也就是得到**每个单词和其他单词的权重分布**
- d~k~ 是特征维度



### 1.1.5、多头注意力机制

多头注意力机制 (Multi-Head Attention) 概念：

- 是Transformer的一个关键组件，它同时使用多个注意力机制来获取多个不同的关注点
- 大白话释义：对文本特征进行切分，**分成多头（相当于分成多个人）去观察**。更加有利于提取事物特征

作用：

- 让多个注意力机制去提取文本特征, 比用1个注意力机制好, **充分提取特征**
- 每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，**让词义拥有来自更多元的表达**，实验表明可以从而提升模型效果

![](黑马Transformer(八).assets/9.png)

1. 线性变换：QKV分别输入到线性层
2. view切分：文本特征做多头切分，比如：256个特征切分成8个头，每个头64个特征
3. attention操作：通过attention函数进行多头特征提取
4. Concat操作：合并多头特征提取结果
5. 线性层变换，最后的得到我们想要的数据形状





### 1.1.6、前馈全连接层

Transformer中前馈全连接层就是具有**两层线性层的全连接网络**。作用：考虑注意力机制可能**对复杂过程的拟合程度不够**, 通过增加**两层网络**来增强模型的能力

![](黑马Transformer(八).assets/1.png)

### 1.1.7、规范化层

是一种用于对输入数据进行归一化的重要组件。它通常紧随着每个子层的输出，以确保网络在处理数据时保持稳定的分布。

- **规范化层实际上是一种归一化的操作，旨在使输入的均值保持接近0，标准差保持接近1**

为什么需要规范化层？

- **防止梯度消失和梯度爆炸**：在深度神经网络中，随着层数的增加，梯度很容易变得非常小（梯度消失）或非常大（梯度爆炸）。规范化层通过将输入数据归一化，**可以缓解这一问题**，使得梯度的传播更加稳定。
- **加速训练过程**：规范化层可使模型在训练时更快地收敛，加速整个训练过程

计算方式：

- 计算**每个位置**的所有特征维度上的均值和方差，然后对该位置的**所有特征维度**进行线性变换，以保证均值为0，方差为1，最后再进行**缩放和平移操作**，引入了可学习的参数（**缩放因子和平移因子**）

- **缩放因子和平移因子**为了学习不同批次之间的数据特征，能让模型有更好的泛化能力



### 1.1.8、编码器层

由N个编码器层堆叠而成，作用是器用于对输入进行指定的特征提取, 也称为编码。





### 1.1.9、解码器层

每个解码器层根据给定的输入向目标方向进行特征提取操作，即解码过程







### 1.1.10、练习

1. 请说出transformer结构中，注意力机制有那几个，分别说一说作用？
   - 一共有3个注意力机制
     - 编码器部分：有**多头自注意力机制**，用来提取源文本的特征信息
     - 解码器部分：**masked的多头自注意力机制**，用来提取目标文本的特征信息
     - 解码器部分：**encode-decode注意力机制**，用来融合编码中间语义张量C

> 注意：**编码是并行编码，解码是一个时间步一个的解码**

2. 请说出transformer结构中，在编码时需要掩码mask，再解码时也需要掩码mask，请说出两者的区别

- **编码时使用mask矩阵**是因为批量给模型送数据，数据不等长需要打补丁变成等长。这样会出现冗余信息。通过**掩码矩阵**在计算注意力机制时，**可以消除冗余单词产生的影响**，处理pad数据
- **解码时mask矩阵**，也就是masked Muti-Head Attention是因为**避免模型生成单词时，看到未来和当下的信息**
- 进一步说：**解码器端**自注意力机制中，模型只能看到当前时间步以前的token。比如已经生成了y1 y2 y3 y4 y5, 要生成y6时先要产生q6。q6要去看一看和已经生成的y1 y2 y3 y4 y5的权重分布。**防止模型看到未来信息**







### 1.1.11、输出部分

包括线性层、softmax层

- 线性层：对上一步的结果**进行指定维度变换**
- softmax层：**转成概率分布**











# 2、fasttext

> 待更新！



















































































