# 1、自然语言处理





## 1.1、自然语言处理概述

自然语言处理（Nature language Processing, NLP）研究的主要是**通过计算机算法来理解自然语言**。对于自然语言来说，处理的数据主要就是人类的语言，例如：汉语、英语、法语等，该类型的数据不像我们前面接触的过的结构化数据(用表格表示)、或者图像数据可以很方便的进行数值化。

NLP应用领域覆盖广泛：

- 涵盖了语音识别、语音合成、自然语言理解、机器翻译、文本分类和情感分析等多个方面
- 比如：自然语言理解让**计算机理解人类语言的语义，在问答系统、智能客服、搜索引擎、智能家居**等场景中有广泛应用







# 2、文本预处理

> 大白话：文本预处理的工作，就是准备出**模型需要的x、y**，然后送给模型。

**文本语料在输送给模型**前一般需要一系列的**预处理工作**，才能符合模型输入的要求

- 比如：将文本转化成模型需要的张量、规范张量的尺寸
- 比如：**关于标签Y**:分类问题查看标签是否均匀；**关于数据X**:数据有没有脏数据、数据长度分布等等

文本预处理的作用就是指导模型超参数的选择、提升模型的评估指标。







## 2.1、文本预处理的主要环节

![](黑马自然语言处理(六).assets/1.png)









## 2.2、文本处理的基本方法

### 2.2.1、分词

- 定义：将**连续的字序列**按照**一定的规范**重新组**合成词序列**的过程

- 作用：词作为语言**语义理解的最小单元**，是人类理解文本语言的基础。是AI解决NLP领域高阶任务，如自动问答，机器翻译，文本生成的重要基础环节
- 中文为什么要分词？中文没有**明显的分解符**，英文天然**空格**是分解符

![image-20250522141635562](黑马自然语言处理(六).assets/2.png)

目前分词的库有很多，比较推荐的是[HanLP](https://hanlp.hankcs.com/)：面向生产环境的前沿多语种自然语言处理技术。但是这里介绍一下过时但是使用起来方便快捷的[jieba](https://github.com/fxsjy/jieba)！

```bash
# 安装
pip install jieba
# 使用
import jieba
```



1. 精确模式分词：试图将句子最精确的切开，适合文本温习

```python
import jieba


# todo: 精确模式分词
content = "我正在进行学习人工智能,好好努力"
result = jieba.lcut(content,cut_all=False)
# ['我', '正在', '进行', '学习', '人工智能', ',', '好好', '努力']
print(result)
```

2. 全模式分词：把句子中所有的词语都扫描出来，速度非常快，但是不能消除歧义

```python
import jieba


# todo: 精确模式分词
content = "我正在进行学习人工智能,好好努力"
result = jieba.lcut(content,cut_all=True)
# ['我', '正在', '进行', '学习', '人工', '人工智能', '智能', ',', '好好', '努力']
print(result)
```

3. 搜索引擎模式分词：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词

```python
import jieba


# todo: 精确模式分词
content = "我正在进行学习人工智能,好好努力"
result = jieba.lcut_for_search(content)
# ['我', '正在', '进行', '学习', '人工', '智能', '人工智能', ',', '好好', '努力']
print(result)
```



> [!note]
>
> 简单解释下提高召回率：
>
> 1. 例如百度后台存储了一条数据doc： [黑马|程序|员]
> 2. 用户搜索的时候，输入一个问题：[黑马程序员]
> 3. 百度使用精确模式对输入的问题进行分词得到query：[黑马|程序员]，query ≠ doc，不能完全匹配，召回率为0
> 4. 百度再次对query使用搜索引擎模式分词得到 query2: [黑马|程序|员]，query ＝ doc，可以完全匹配，召回率为1





4. 自定义词典：添加自定义词典后，jiba能够准确识别词典中出现的词汇，提升整体的识别准确率。
   - 词典格式：每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。

```txt
正在进行 5 n
好好努力 4 n
人工智能 7 nz
```

```python
import jieba


# todo: 精确模式分词
content = "我正在进行学习人工智能,好好努力"
# 1、没有使用用户自定义词典
result = jieba.lcut(content,cut_all=False)
# ['我', '正在', '进行', '学习', '人工智能', ',', '好好', '努力']
print(result)


# 2、使用用户自定义词典
jieba.load_userdict("./userdict.txt")
result2 = jieba.lcut(content,cut_all=False)
# ['我', '正在进行', '学习', '人工智能', ',', '好好努力']
print(result2)
```

> jieba词性对照表：
>
> ![](黑马自然语言处理(六).assets/3.png)





### 2.2.2、命名实体识别

- 命名实体：将人名，地名，机构名等专有名词统称命名实体。如：周杰伦，黑山县，孔子学院

- 命名实体识别：(Named Entity Recognition,简称NER),识别出一段文本中可能存在的命名实体
- 作用：命名实体也是**人类理解文本的基础单元**，是AI解决NLP领域高阶任务的重要基础环节

![image-20250522141606265](黑马自然语言处理(六).assets/4.png)



### 2.2.3、词性标注

- 词性：语言中**对词的一种分类方法**，以语法特征为主要依据、兼顾词汇意义对词进行划分的结果。常见的词性有14种，如：名词，动词，形容词
- 词性标注：(Part-Of-Speech tagging,简称POS),标注出一段文本中每个词汇的词性
- 词向标注作用：对**文本语言的另一个角度的理解**，A解决NLP领域高阶任务的重要基础环节

![](黑马自然语言处理(六).assets/5.png)

```python
import jieba.posseg as pseg


# todo: 精确模式分词
content = "我正在进行学习人工智能,好好努力"
result = pseg.lcut(content)
# 每个元组由词汇和词性组成
# [pair('我', 'r'), pair('正在', 't'), pair('进行', 'v'), pair('学习', 'v'), pair('人工智能', 'n'), pair(',', 'x'), pair('好好', 'd'), pair('努力', 'ad')]
print(result)
```















## 2.3、文本张量

- 定义：将一段文本**使用张量进行表示**这个过程就是文本张量表示。**词表示成向量叫词向量，那么一句话构成词向量矩阵**
- 作用：将文本表示成张量（矩阵）形式，方便输入到计算机程序中进行解析

![](黑马自然语言处理(六).assets/6.png)

### 2.3.1、One-hot编码

NLP中文本词向量的表示常用方法：

1. one-hot编码：也叫**稀疏词向量表示**
   - 是将**分类变量**转换为**数字格式**的常用方法，通常用于AI任务中处理**分类标签y数据**
   - 在One-hot编码中，对于一个具有n个不同类别的分类向量，将其表示为一个n维的向量，其中**只有一个维度的值为1**(代表该样本属于这个类别)，其他维度的值均为0
   - 记忆：**一个位置为1，其他全为0**

![](黑马自然语言处理(六).assets/7.png)

优缺点

- 优点：操作简单，容易理解
- 缺点；完全割裂了词与词之间的联系；大语料集下，每个向量的长度过大，占据大量内存。属于稀疏词向量表示
- 抛砖：正因为one-hot编码隔离了词和词的联系，又易浪费内存空间，出现了：**稠密向量的表示方法**：word2vec 和vord embedding









### 2.3.2、Word2Vec模型

Word2Vec慨念：

- 是一种将**单词**转换为**词向量**的自然语言处理技术
- 是利用**深度学习网络**来探索**单词和单词之间的语义关系**，用深度学习的**网络权重参数**表示词向量
- 是在**无监督的语料**上构建了一个**有监督的任务**

Word2Vec有两种训练词向量方式：

- **CBOW**(Continuous Bag of Words)方式训练词向量：试图根据上下文中的周围单词来预测当前单词（简言之：**两侧预测中间**）它将周围单词的词向量求和或取平均作为上下文的表示，然后通过一个神经网络进行预测。
- **Skip-gram**方式训练词向量：试图根据当前单词来预测上下文中的周围单词（简言之：**中间预测两侧**）它将当前单词的词向量作为输入，然后通过一个神经网络来预测周围单词

#### 1、CBOW

![](黑马自然语言处理(六).assets/8.png)





CBOW模式下的word2vec过程说明：

假设我们给定的训练语料只有一句话：

1. Hope can set you free(愿你自由成长)，窗口大小为3,因此模型的第一个训练样本来自Hope can set,因为是CBOW模式，所以将使用Hope和set作为输入，can作为输出。
2. 在模型训练时，Hope、can、set等词汇都使用他们的one-hot编码，每个one-hot编码的词与各自的变换矩阵(即模型的初始化权重参数矩阵)相乘之后再相加，得到上下文语义矩阵
3. 将上下文语义矩阵与变换矩阵相乘，得到结果矩阵，将结果矩阵和 can 的one-hot 编码矩阵进行 SoftMax 损失计算，然后反向传播更新参数矩阵完成一次模型迭代。

> 其实有没有发现，就是一个多分类的问题，我们需要将 can 插入到哪两个单词中间呢？这就是一个典型的多分类问题！

1. 例如我们要对【我爱黑马程序员啊】这句话进行训练词向量，首先进行分词，得到词及其one-hot编码

![](黑马自然语言处理(六).assets/9.png)



2. 对上述原始分词文本选取窗口，我们这边选取窗口3。我们再规定每个词向量的维度为3(3行1列)，这个词向量的维度可以随意更改，4、5都可以。
   - 我们训练第一个样本：我 | 爱 | 黑马，输入是 **我、黑马**，输出是 **爱**
   - 将**我、黑马**的 one-hot 编码和我们**模型的初始化权重参数**相乘再相加，得到上下文的中间语义
   - Tips：这里为什么说模型初始化的权重参数矩阵的每一列可以理解成是分词的词向量呢？因为其实发现每个分词的 one-hot 编码和权重矩阵相乘得到的依然是权重矩阵的列(如图中的红色列)

![](黑马自然语言处理(六).assets/10.png)

3. 将**我、黑马**的 one-hot 编码和我们**模型的初始化权重参数**相乘再相加得到的值继续和模型初始化权重参数相乘，得到的是概率值矩阵，将概率值矩阵和真实的热编码进二者进行 Softmax 交叉熵损失，得到损失最少的值。

![](黑马自然语言处理(六).assets/11.png)

4. 窗口按序向后移动，重新更新参数，直到所有语料被遍历完毕，得到最终的模型初始化权重参数矩阵3×5，这个矩阵就是词向量。将这个模型初始化权重参数矩阵3×5和分词的one-hot编码矩阵5×1相乘，得到的3×1的矩阵就是该词汇的 word2vec 张量表示。

> 总结：
>
> 1. 神经网络输入5个特征，经过隐藏层变成3个特征，经过输出层变成5个特征。
> 2. 数据经过前向传播得到预测值y',根据预测值y'和真实值y比较得到损失
> 3. 通过反向传播可以更新两个模型初始化权重参数矩阵







#### 2、Skingram

Skingram模式下的word2vec过程说明：

假设我们给定的训练语料只有一句话：

1. Hope can set you free(愿你自由成长)，窗口大小为3,因此模型的第一个训练样本来自Hope can set,因为是CBOW模式，所以将使用can作为输入，Hope和set作为输出。
2. 在模型训练时，Hope、can、set等词汇都使用他们的one-hot编码，每个one-hot编码的词与各自的变换矩阵(即模型的初始化权重参数矩阵)相乘之后再相加，得到上下文语义矩阵
3. 将上下文语义矩阵与变换矩阵相乘，得到结果矩阵，将结果矩阵和 can 的one-hot 编码矩阵进行 SoftMax 损失计算，然后反向传播更新参数矩阵完成一次模型迭代。











### 2.3.3、词向量的检索获取

神经网络训练完毕后，神经网络的参数矩阵就是我们的想要词向量。如何检索某1个单词的词向量呢？以CBOW方式举例说明如何检索词向量。
如下图所示：**我**的one-hot编码[10000]，用参数矩阵[3,5]***我**的onehot编码[10000l,可以把参数矩阵的第1列参数给取出来，这个[3,1]的值就是**我**的词向量。



### 2.3.4、skipgram和cbow

CBOW(Continuous Bag of Words)和Skip-gram是Word2Vec模型的两种主要实现方式，它们分别从不同角度来建模单词之间的关系。以下是它们之间的一些比较：

CBOW(Continuous Bag of Words)模型原理：

- CBOW模型试图根据上下文中的周围单词来预测当前单词。它将周围单词的词向量求和或取平均作为上下文的表示，然后通过一个神经网络进行预测。
- 计算效率：CBOW相对于Skip-gram在计算上更为高效，因为它将多个上下文单词的词向量求和或取平均，减少了模型的复杂度。
- 数据需求：CBOW通常需要比Skip-gram更多的训练数据才能取得好的效果，因为它对上下文的整体信息进行了汇总。
- 适用场景：CBOW在训练数据较大的情况下往往表现较好，特别是在低频词的情况下。



Skip-gram:模型原理：

- Skip-grm模型试图根据当前单词来预测上下文中的周围单词。具体来说，它将当前单词的词向量作为输入，然后通过一个神经网络来预测周围单词。
- 灵活性：skip-gram相对于CBOW在灵活性上更为强大，因为它将单词的上下文信息进行了明确建模，可以处理更复杂的语义关系。
- 低频词处理：Skip-grm在处理低频词时往往更具优势，因为它能够生成更丰富的上下文信息，尤其是在大规模数据集中。
- 训练速度：相对于CBOW,Skip-gram的训练速度通常**较慢**，因为它需要对每个单词生成上下文。

总结比较：

- CBOW适合，**相对来说原理简单**，尤其是高频词的情况下，它的计算效率高。
- Skip-gram对于低频词的处理更为出色，尤其在大规模数据集中，但相对计算效率较低。
- CBOW将多个上下文单词汇总起来，能够快速生成词向量，但可能会丧失一些细节信息。
- Skip-grm明确地建模了单词的上下文信息，因此在复杂的语义关系中表现更好，但训练速度相对较慢。
- 总的来说，选择CBOW或Skip-gram取决于具体的任务需求、数据集特性以及对计算资源的要求。







### 2.3.5、WordEmbedding

上述word2vec训练的词向量是静态词向量，也就是word2vec产生词向量后，词向量就固定下来了，是静态的。而 WordEmbedding 是动态词向量，产生的词向量会变化。

相同点：都可以用来对文本训练词向量。

- word2vec 产生词向量后，**词向量就固定下来了，是静态的**。

  - word2vec使用起来一般**需要两步**：1、输入单词 the 拿到词向量。 2、再送给神经网络进行使用	

- WordEmbedding 是指**在深度神经网络中嵌入一个层**，nn.Embedding 层产生词向量后，词嵌入层作为神经网络的一部分，**权重参数会参与更新，是动态的**。

  - nn.Embedding 层使用起来**就一步**：直接嵌入到神经网络中，更易使用

  

  

  

  

  

  

## 2.4、文本数据分析

文本语料数据分析的作用：

- 文本数据分析能够有效帮助我们**理解数据语料**，快速检查出**语料可能存在的问题**，指导模型训练过程中一些**超参数的选择**
- 比如：**关于标签Y**:分类问题查看标签是否均匀；**关于数据X**:数据有没有脏数据数据长度分布等等

常用的几种文本数据分析方法：

- 标签数量分布
- 句子长度分布
- 词频统计与关键词词云



### 2.4.1、标签数量统计

什么是标签数量分布：即求标签0有多少个，标签1有多少个，标签2有多少个等等。



假如有一个`1.xlsx`文件，里面有两列，两列的标题分别为sentence和label

- 第一列数据sentence代表有感情色彩的评论文本
- 第二列数据，0或1，代表评论的文本是好评还是差评，0代表差评，1代表好评



```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1、读取Excel文件
# 文件名为1.xlsx，工作表名为Sheet1
df = pd.read_excel('./data/1.xlsx', sheet_name='Sheet1')

# 2、统计每个类别的出现次数,x轴为1.xlsx文件的label,数据是读取的excel的数据
sns.countplot(x='label',data=df)


plt.show()
```

![](黑马自然语言处理(六).assets/12.png)

训练深度学习模型比如分类问题，一般需要**将正负样本比例维持在1:1左右**

- 如上图中的0和1的比例为1:1，不符合这个比例，需进行**数据增强或删减**





### 2.4.2、句子长度分布

句子长度就是1.xlsx文件的sentence的列的文字长度，例如长度为3的有多少个，长度为5的有多少个等等。

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1、读取Excel文件
# 文件名为1.xlsx，工作表名为Sheet1
df = pd.read_excel('./data/1.xlsx', sheet_name='Sheet1')

# 2、新增句子长度列 sentence_length,计算1.xlsx文件中sentence列的长度
df['sentence_length'] = df['sentence'].str.len().tolist()

# 查看读取的1.xlsx的前几列
print(df.head())
# 3、绘制数据长度分布图-柱状图
sns.countplot(x='sentence_length',data=df)
plt.show()

# 4、绘制数据长度分布图,曲线图
sns.displot(x='sentence_length',data=df)
plt.show()
```

![](黑马自然语言处理(六).assets/13.png)

若模型对输入的数据长度有要求，可以对句子进行**截断或补齐操作**，(规范长度)起到关键的指导作用



### 2.4.3、正负样本长度散点分布

正负样本长度散点分布：也就是0标签的句子长度，1标签的句子长度。



```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1、读取Excel文件
# 文件名为1.xlsx，工作表名为Sheet1
df = pd.read_excel('./data/1.xlsx', sheet_name='Sheet1')

# 2、新增句子长度列 sentence_length,计算1.xlsx文件中sentence列的长度
df['sentence_length'] = df['sentence'].str.len().tolist()

# 调用散点图(x轴为xlsx文件的label列,y轴为xlsx的sentence_length列)
sns.stripplot(x='label',y='sentence_length',data=df)
plt.show()
```

![](黑马自然语言处理(六).assets/14.png)

通过查看正负样本长度散点图, 可有效定位异常点的出现位置, 帮助我们更准确进行人工语料审查。例如有可能突然出现异常的散点图,表明这个评论sentence的长度过大或者过小。







### 2.4.4、词汇总数

词汇总数也就是对训练的词列表进行**去重并分词**，统计出不同词汇的总数。

例如我们对 1.xlsx 文件中的sentence列的句子进行分词去重,然后统计共有多少个词汇，这个就是词汇总数。

```python
import pandas as pd
import jieba

# 1、读取Excel文件
# 文件名为1.xlsx，工作表名为Sheet1
df = pd.read_excel('./data/1.xlsx', sheet_name='Sheet1')


# 定义生成器函数,用于逐句处理数据集中 sentence 列的每一行,并使用结巴分词对每个句子进行分词，然后逐个生成分词结果
def segment_generator(data):
    for sentence in data['sentence']:
        yield from jieba.lcut(sentence)


# 使用set去重分词得到的结果
train_vocab = set(segment_generator(df))

print("句子中的词汇都有:",train_vocab)
print("训练集共包含不同词汇总数为:",len(train_vocab))
```

![](黑马自然语言处理(六).assets/15.png)



## 2.5、文本特征处理

文本特征处理：给我们的语料添加**具有普适性的文本特征**，让模型更有效的处理数据，提高模型性能指标。例如句子长度我认为20-50长度才有用，那么我们对原本的数据会进行增加或者删除。

常用方法：

- **添加** n-gram 特征(**两个单词总是相邻并共现**，可以认为是一个特征)
- **规范** 文本长度规范(**文本的长度是多少，也可以认为是一个特征**)

### 2.5.1、n-gram特征

给定一段文本序列, 其中**n个字或词的相邻共现**，可以做为一个特征，即n-gram特征，常用的n-gram特征是bi-gram(2-gram)和tri-gram(3-gram)特征, 分别对应n为2和3。

![](黑马自然语言处理(六).assets/16.png)

> 原始语料添加上更具有普适性的特征，然后再送给模型

例子：计算一个文本序列有多少个**2-gram**特征。

- 例如如下文本序列， [1, 3, 2, 1, 5, 3]，其中2-gram特征有(1,3)、(3,2)、\(2,1)、(1,5)、(5,3)

```python
from typing import List, Set, Tuple, Any


# 封装计算序列有多少个 ngram 特征
# 两个参数:1、输入的列表数据  2、n为2还是3
def generate_ngrams(
        token_sequence: List[Any],
        ngram_size: int
) -> Set[Tuple[Any, ...]]:
    """
    从令牌序列生成n-gram集合

    参数:
        token_sequence: 输入令牌序列 (列表)
        ngram_size: n-gram的大小 (正整数)

    返回:
        包含所有唯一n-gram的集合，每个n-gram表示为元组
    """
    if ngram_size <= 0:
        raise ValueError("ngram_size必须为正整数")

    if len(token_sequence) < ngram_size:
        # 处理短于n的序列
        if token_sequence:
            return {tuple(token_sequence)}
        return set()

    ngrams = set()
    # 计算有效的起始索引范围
    max_start = len(token_sequence) - ngram_size + 1

    for start_idx in range(max_start):
        # 提取当前n-gram
        ngram_tuple = tuple(token_sequence[start_idx:start_idx + ngram_size])
        ngrams.add(ngram_tuple)

    return ngrams


input_list = [1, 3, 2, 1, 5, 3]
ngram_range = 2 # 一般取2或3
ngrams = generate_ngrams(input_list, ngram_range)
print(ngrams)
# {(2, 1), (1, 5), (5, 3), (3, 2), (1, 3)}
```





### 2.5.2、文本长度规范

- 送给模型的数据一般都是**长度要求的**；比如批量（每次送8个样本）样本长度要一样，这样需要对批量数据进行文本长度规范
- 比如：**文本过长需要截断，文本过短需要打pad补齐（补零）**这个操作就是文本长度规范





## 2.6、文本数据增强

一般基于google/百度/获取其他翻译接口，将文本数据翻译成另外一种语言(一般选择小语种),之后再翻译回原语言，即可认为得到与与原语料同标签的新语料。

- **新语料**加入到原数据集中即可认为是对原数据集**数据增强**(这个操作有可能对模型有用，也有可能没用)
- 存在问题：短文本回译中, 新语料与原语料可能存在很高的**重复率**, 并不能有效增大样本的特征空间
- 高重复率解决办法：连续的多语言翻译
  - 中文 → 韩文 → 日语 → 英文 → 中文，一般不超过3次
  - 更多的翻译次数将产生效率低下, **语义失真**等问题





# 3、RNN模型

循环神经网络RNN(Recurrent Neural Network，简称RNN)，一般以**序列数据**为输入, 通过**网络内部的结构设计**有效捕捉序列之间的关系特征, 一般也是以**序列形式进行输出**

![](黑马自然语言处理(六).assets/17.png)



> 为什么叫循环什么网络?
>
> - 上一时间步的**隐藏层输出**作为下一个时间步的**隐藏层输入**
>
> ![](黑马自然语言处理(六).assets/1.gif)

以时间步对RNN进行展开后的单层网络结构：

![](黑马自然语言处理(六).assets/2.gif)



RNN网络结构的特点：

- 由输入层、隐藏层、输出层组成，**上一时间步的隐藏层输出**作为下一个**时间步的隐藏层输入**

- 每个时间步的输入有2个：**数据端输入(上方的红点)，隐藏层输入(上方的蓝点)**
- 每个时间步的输出有2个：**数据端输出(上方的黑点)，隐藏层输出(上方的蓝点)**

RNN模型的作用：

- RNN结构能够**连续性的输入序列数据**，进行特征提取。比如：人类的语言, 语音特别适合RNN进行处理
- RNN模型广泛应用于NLP领域的各项任务, 如文本分类, 情感分析, 意图识别, 机器翻译等。

我们来举个例子：例如我们给AI输入 `What time is it?` ，AI来理解我们的意图

![](黑马自然语言处理(六).assets/3.gif)



1. 第一个时间步:AI先进行分词，首先将 What 输送给RNN，它将产生一个**输出O1**
2. 第二个时间步：继续将单词 Time 输送给 RNN，RNN不仅利用 time 来产生**输出O2**，还使用上一层**隐藏层输出O1**作为输入信息
3. 重复这样的步骤，直到处理完所有的单词。**最终的隐藏层输出O5就是整个句子的语义，因为O5就包含了O1-O4的信息**。
   - 但是这样有一个问题，如果时间步非常长，就会丢失前面的输出层的语义。

4. 最后将O5输出结果经过softmax层转换成分类概率分布，如果意图识别是18个分类，就进行softmax18，得到概率最大的就是AI理解的意图



## 3.1、RNN模型的分类

1. 按照**输入和输出**的角度进行分类
   - 输入N个序列，输出N个序列：例如写诗、写对联
   - 输入N个序列，输出1个的值：例如情感分类、意图识别
   - 输入1个序列，输出N个的值：例如看图说话
   - 输入N个序列，输出M个序列：例如翻译、语音转换、文本生成

![](黑马自然语言处理(六).assets/18.png)

![](黑马自然语言处理(六).assets/19.png)



2. 按照**RNN的内部构造**进行分类

- 传统RNN
- LSTM
- Bi-LSTM
- GRU
- Bi-GRU



### 3.1.1、传统RNN模型

RNN内部结构图如下：

![](黑马自然语言处理(六).assets/20.png)



1. 每个**时间步有2个输入**：h(t-1)上一时间步的隐藏层输出， x(t)当前时间步的输入
2. 进入RNN结构体后，上述两个输入会融合到一起，通过一个**全连接层(线性层)**，再使用 tanh 作为激活函数。
   - 全连接层(线性层)：其实就是矩阵运算，例如输入10个特征，输出8个特征，其实就是进行了矩阵变换。

3. 每个时间步有2个输出：h(t)该时间步的隐藏层输出（它将作为下一个时间步的隐藏层输入），x(t+1) 当前时间步的输出
4. 在编程时：字母如下

![](黑马自然语言处理(六).assets/22.png)

> [!note]
>
> - 在学术界中RNN内部的计算公式为：
>   - 其实也就是让**上一时间步的隐藏层输出和当前时间步的输入** 融合，然后相乘矩阵参数W~t~并加偏置b，再经过tanh激活函数
>
> $$
> h_t = tanh(W_t[X_T,h_t-1]+b_t)
> $$
>
> - 在Pytorch中是有两个矩阵参数：
>   - 其实也就是让上一时间步的隐藏层输出和矩阵参数W~1~相乘并加偏置b，当前时间步的输入和矩阵参数W~2~相乘并加偏置b，再经过tanh激活函数
>
> $$
> h_t = tanh(W_1X_T+b_1+W_2h_{t-1}+b_2)
> $$
>
> 
>
> 另外，tanh激活函数的作用就是将值压缩在[-1,1]之间

### 3.1.2、Pytorch构建RNN模型

例子：假如我们有3句话，每句话有1个单词，每个单词有5个特征。我们希望经过RNN模型的处理，使得每个单词有6个特征。RNN模型构造过程中有9个参数：

![](黑马自然语言处理(六).assets/21.png)



如上图：

- 输入数据：3个批次，每个批次1个单词，每个单词5个特征
- 模型：规定RNN模型输入5个特征，输出6个特征，隐藏层数量为1
- 输出数据：3个批次，每个批次1个单词，每个单词6个特征

```python
import torch
import torch.nn as nn

# 规定RNN模型的输入数据是5个特征、输出数据是6个特征(可以认为是6个神经元)、隐藏层个数为1
rnn = nn.RNN(5,6,1)
# 输入给RNN模型的数据是3个批次、每个批次1个单词、每个单词5个特征
input = torch.randn(1,3,5)
# h0输入数据为3个批次、每个批次1个单词、每个单词6个特征
h0 = torch.rand(1,3,6)
# 给模型送数据
# 数据形状 input[1,3,5] h0[1,3,6] ---> output[1,3,6],hn[1,3,6]
output,hn = rnn(input,h0)
# 打印输出
print('output-->', output.shape, output)
print('hn--->', hn.shape, hn )
```

注意：

- h0和hn的形状完全一致！
- 当隐藏层个数为1时，output结果和hn结果输出是一样的
- 当隐藏层个数为n时，output结果和最后一个隐藏层输出hn结果是一样的

- 在上面我们填写参数时，发现批次3在第二个参数，这和我们的习惯不一致，我们能不能将3批次写在第一个参数呢？完全可以，只需要在创建RNN模型的时候加上` batch_first=True`,这个只会对 input 和 output 的形状有影响，对h0、hn无影响。

```python
import torch
import torch.nn as nn

# 规定RNN模型的输入数据是5个特征、输出数据是6个特征(可以认为是6个神经元)、隐藏层个数为1
rnn = nn.RNN(5,6,1, batch_first=True)
# 输入给RNN模型的数据是3个批次、每个批次1个单词、每个单词5个特征
input = torch.randn(3,1,5)
# h0输入数据为3个批次、每个批次1个单词、每个单词6个特征
h0 = torch.rand(1,3,6)
# 给模型送数据
# 数据形状 input[3,1,5] h0[1,3,6] ---> output[3,1,6],hn[1,3,6]
output,hn = rnn(input,h0)
# 打印输出
print('output-->', output.shape, output)
print('hn--->', hn.shape, hn )
```





### 3.1.3、LSTM模型

LSTM（Long Short-Term Memory）也称长短时记忆结构，它是传统RNN的变体，与经典RNN相比，能够**有效捕捉长序列**之间的语义关联, **缓解梯度消失或爆炸现象**。

![](黑马自然语言处理(六).assets/23.png)



LSTM内部结构：

- 由输入层、隐藏层、输出层组成
- 每个时间步有**三个输入**：数据端输入X~t~，上一个时间步细胞状态C~t-1~，上一个时间步的h~t-1~
- 每个时间步有**三个输出**：数据端输出，本时间步细胞状态C~t~，本时间步的h~t~
- LSTM结构更加复杂，内部有3个门+1个细胞状态：**遗忘门、输入门、细胞状态、输出门**

优点：

- LSTM的门结构能够**有效减缓**长序列问题中可能出现的梯度消失或爆炸，虽然并不能杜绝这种现象，但在更长的序列问题上表现优于传统RNN

缺点：

- 由于内部结构相对较复杂，因此训练效率在同等算力下较传统RNN**低很多**

---

面试问答题：**LSTM是如何实现记忆信息的**？

1. LSTM引进门控机制和细胞状态，也就是**内部记忆单元**。
2. 在⼀个训练好的网络中，当输入的序列中没有重要的信息时，LSTM的遗忘门的数值接近于1，输入门的数据接近于0，此时过去的记忆会被保存，从而实现了长期的记忆功能
3. 当输入的序列中**出现了重要的信息**时，LSTM应该把其存入记忆时，此时**输出门的数值将接近于1**
4. 所以**引入各种门机制（遗忘门 输入门 输出门，细胞状态）的确可以控制当前时间步、以前时间步的信息谁重要谁不重要**。从而实现LSTM(长短时记忆网络)



### 3.1.4、GRU模型

GRU（Gated Recurrent Unit）也称门控循环单元结构

- 它也是传统RNN的变体
- 同LSTM一样，能够**有效捕捉长序列**之间的语义关联, **缓解梯度消失或爆炸现象**
- 同时它的结构比LSTM要简单，比RNN要复杂

![](黑马自然语言处理(六).assets/24.png)

优点：

- GRU和LSTM作用相同, 在捕捉长序列语义关联时, 能有效抑制梯度消失或爆炸, 效果都优于传统RNN，且**计算复杂度相比LSTM要小**

缺点：

- GRU仍然不能完全解决梯度消失问题, 同时其作用RNN的变体, 有着RNN结构本身的一大弊端,**即不可并行计算**, 这在数据量和模型体量逐步增大的未来, 是RNN发展的关键瓶颈



# 4、案例：人名分类器

> 待更新！





















































