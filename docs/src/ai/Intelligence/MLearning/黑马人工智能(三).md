# 5、集成学习

![](黑马人工智能(三).assets/1.png)



机器学习的两个核心任务：

- 任务一：**如何优化训练数据** —> 主要用于**解决欠拟合问题**
- 任务二：**如何提升泛化性能** —> 主要用于**解决过拟合问题**

![](黑马人工智能(三).assets/2.png)



![](黑马人工智能(三).assets/31.png)



## 5.1、Bagging

我们来看一下Bagging的基本过程，例如我们要把圈和方块分类，达到左侧全是圆圈，右侧全是方块。(即使有个别错误也无伤大雅)

![](黑马人工智能(三).assets/3.png)

Bagging实现过程：

1. 首先Bagging会采集圆和方块，采集不同的数据集
2. 根据采集到的数据集来训练分类器

![](黑马人工智能(三).assets/4.png)



3. 平权投票，获取最终结果。假如又过来一个样本，我们看在三个分类器的那部分，然后将其聚合

![](黑马人工智能(三).assets/5.png)



## 5.2、随机森林

在机器学习中，**随机森林是一个包含多个决策树的分类器**，并且其输出的类别是由个别树输出的类别的众数而定。

![](黑马人工智能(三).assets/6.png)

例如, 如果你训练了5个树, 其中有4个树的结果是True, 1个树的结果是False, 那么最终投票结果就是True



随机森林够造过程中的关键步骤（M表示特征数目）：

1. 一次随机选出一个样本，有放回的抽样，重复N次（有可能出现重复的样本）

2. 随机去选出m个特征, m <<M，建立决策树



### 5.2.1、随机森林API

待更新！



## 5.3、Boosting

![](黑马人工智能(三).assets/7.png)



实现过程：

1. Boosting训练第一个学习器：随便分一下，发现有分错的。就会调整数据分布，把错误的数据放大，把正确的数据缩小。
2. Boosting训练第二个学习器：会更多注意于错误的放大数据，再次调整数据分布，再重复放大缩写步骤

![](黑马人工智能(三).assets/8.png)







![](黑马人工智能(三).assets/9.png)





![](黑马人工智能(三).assets/10.png)













# 6、聚类算法

**使用不同的聚类准则，产生的聚类结果不同**。

**聚类算法**：

- 一种典型的**无监督**学习算法，主要用于将相似的样本自动归到一个类别中。在聚类算法中根据样本之间的相似性，将样本划分到不同的类别中，对于不同的相似度计算方法，会得到不同的聚类结果，一般使用欧式距离来计算相似度。
- 聚类算法的目的是在**没有先验知识**的情况下，自动发现数据集中的**内在结构和模式**。

![](黑马人工智能(三).assets/32.png)

聚类算法分类：

1. 根据聚类颗粒度分类：粗聚类(聚类个数少)、细聚类(聚类个数多)
2. 根据实现方法分类: k-means聚类、谱聚类



聚类算法与分类算法最大的区别：**聚类算法是无监督的学习算法，而分类算法属于监督的学习算法**。

```python
# 设置8个聚类个数
sklearn.cluster.KMeans(n_clusters=8)
```

- n_clusters:开始的聚类中心数量
- estimator.fit_predict(x)，计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(x),然后再调用predict(x)

随机创建不同二维数据集作为训练集，并结合k-means算法将其聚类，你可以尝试分别聚类不同数量的簇，并观察聚类效果：

![](黑马人工智能(三).assets/13.png)

```python
# 导入必要的库
import matplotlib.pyplot as plt  # 数据可视化
from sklearn.datasets import make_blobs  # 生成模拟数据
from sklearn.cluster import KMeans  # K-means聚类算法
from sklearn.metrics import calinski_harabasz_score  # 聚类效果评估指标

# 生成模拟数据集
# n_samples: 样本量 1000
# n_features: 特征维度 2（二维数据，方便可视化）
# centers: 指定4个数据中心点坐标
# cluster_std: 每个簇的标准差（控制簇的紧密程度）
# random_state: 随机种子保证可重复性
X, y = make_blobs(n_samples=1000, n_features=2, 
                  centers=[[-1, -1], [0, 0], [1, 1], [2, 2]],
                  cluster_std=[0.4, 0.2, 0.2, 0.2],
                  random_state=9)

# 可视化原始数据集（未聚类）
# plt.scatter(X[:, 0], X[:, 1], marker='o')
# plt.title("Raw Data Distribution")
# plt.show()

# 执行K-means聚类（此处测试n_clusters=2的情况）
# random_state保证每次初始化中心点一致
y_pred = KMeans(n_clusters=2, random_state=9).fit_predict(X)

# 可视化聚类结果（进行聚类）
# c=y_pred表示用聚类标签作为颜色编码
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.title("K-means Clustering Results (n_clusters=2)")
plt.show()

# 使用Calinski-Harabasz指数评估聚类效果
# 该指标通过类内离散度与类间离散度的比值评估聚类质量
# 分数越高表示聚类效果越好
print("Calinski-Harabasz Score:", calinski_harabasz_score(X, y_pred))
```

![](黑马人工智能(三).assets/12.png)







## 6.1、K-means聚类步骤

1. 事先确定常数K，常数K意味着最终的聚类类别数
2. 随机选择**K个样本点**作为初始聚类中心
3. 计算**每个样本**到K个中心的距离，选择最近的聚类中心点作为标记类别
4. 分成K簇后，重新计算出K个簇的**新的聚类中心点**(平均值)，如果计算得出的**新中心点和原中心点**一样则停止聚类，否则重新进行第2步过程，直到聚类中心不再变化。

通过下图解释实现流程：

1. 随机选择3个点作为聚类中心(图1右)
2. 算每个点到这三个点的距离，选择距离最近的标记未相同的类别(图2右)
3. 全部标记完成后，再选取一个新中心点，如果新中心点和第一步选取的中心点一样，那么结束。否则重新进行第二步(图3右)

![](黑马人工智能(三).assets/14.png)

看一下动态效果图：

![](黑马人工智能(三).assets/15.png)



---

来看一个案例，假如我们有15个点，

![](黑马人工智能(三).assets/16.png)

1. 随机设置K个特征空间内的点作为初始的聚类中心（本案例中设置p1和p2），根据欧式距离来计算距离
   - 分别计算p3-p15到p1和p2的距离，然后归为p1类或者p2类



![](黑马人工智能(三).assets/17.png)

如上图，分成了两簇，中心点为P1簇和P2簇。

2. 接着对两个簇，重新计算出每个簇的新中心点（平均值）
   - 得到p1'和p2',发现和p1、p2并没有重合，则重新进行上一步，算距离再聚类

![](黑马人工智能(三).assets/18.png)



形成新的P~1~'为中心的簇和P~2~'为中心的簇

3. 当每次迭代结果不变时，认为算法收敛，聚类完成，**K-Means一定会停下，不可能陷入一直选质心的过程。**(质心重合太过完美，所以平常也可以设置一个阈值，当达到阈值即停止)

![](黑马人工智能(三).assets/19.png)





## 6.2、模型评估

### 6.2.1、误差平方和SSE

误差平方和SSE：就是(真实值-预测值)^2^求和的过程。

![](黑马人工智能(三).assets/20.png)

在K-means中的应用为：
$$
SSE = \sum_{i=1}^{k} \sum_{p∈C_i}|p-m_i|^2
$$
解释下公式:

- |p-m~i~|^2^ 就是各个点到聚类中心点的距离的平方
  - p表示簇内的某个样本，m表示质心点

- 每个簇的样本点到当前簇聚类中心点的距离的平方求和。C~i~表示簇，k表示聚类中心的个数
- k是类别，下图总共分为两类

![](黑马人工智能(三).assets/21.png)

- SSE随着聚类迭代,其值会越来越小,直到最后趋于稳定，**SSE越小，表示数据点越接近它们的中心，聚类效果越好**:
- SSE是对簇内中心点和样本点距离平方求和，再对k个簇求和。

- 如果质心的初始值选择不好,SSE只会达到一个不怎么好的局部最优解.



### 6.2.2、肘方法 - K值确定

肘方法主要是为了帮我们确定K值，也就是分成几簇。横坐标是类别数，表示聚类成几类：

![](黑马人工智能(三).assets/23.png)

1. 对于n个点的数据集，迭代计算k从1到n簇，每次聚类完成后计算每个点到其所属的簇中心的距离的平方和。**随着k越来越大，SSE值越来越小。**
2. 平方和是会逐渐变小的，直到k==n时平方和为0，因为每个点都是它所在的簇中心本身。
3. 在这个平方和变化过程中，会出现一个拐点也即**肘点**，**下降率突然变缓时即认为是最佳的k值**。

在决定什么时候停止训练时，肘形判据同样有效，数据通常有更多的噪音，在**增加分类无法带来更多回报时，我们停止增加类别**。





### 6.2.3、SC轮廓系数法

- 轮廓系数法考虑簇内的内聚程度，簇外的分离程度。
  - 计算每一个样本 i 到同簇内其他样本的平均距离a~i~ ，该值越小，说明簇内相似程度越大。
  - 计算每一个样本 i 到其他簇 j 内的所有样本的平均距离 b~ij~ ，该值越大，说明该样本越不属于其他簇。


$$
S = \frac{b-a}{max(a,b)} \\
a是样本i到簇内其他点的距离平均值 \\
b是样本i到其他簇的样本点的距离平均值的最小值
$$

根据上述公式计算所有样本的轮廓系数，轮廓系数的范围是:[-1,1]，SC轮廓系数值越大聚类效果越好。





## 6.3、K-means算法优化

**K-means算法优点：**

 1.原理简单（靠近中心点），实现容易

 2.聚类效果中上（依赖K的选择）

**K-means算法缺点：**

1. 对离群点，噪声敏感 （中心点易偏移）
2. 很难发现大小差别很大的簇及进行增量计算
3. 结果不一定是全局最优，只能保证局部最优（与K的个数及初值选取有关）

### 6.3.1、K-means++

$$
P = \frac{D(x)^2}{\sum{D(x)^2}}
$$



- D(x)就是其他点到聚类中心点的距离平方

![](黑马人工智能(三).assets/24.png)

如下图中，如果第一个质心选择在圆心，那么最优可能选择到的下一个点在P(A)这个区域（根据颜色进行划分，也就是红色最外侧）

![](黑马人工智能(三).assets/25.png)







### 6.3.2、二分k-means

![](黑马人工智能(三).assets/26.png)

因为聚类的误差平方和能够衡量聚类性能，该值越小表示数据点越接近于他们的质心，聚类效果就越好。所以需要对误差平方和最大的簇进行再一次划分，因为误差平方和越大，表示该簇聚类效果越不好，越有可能是多个簇被当成了一个簇，所以我们首先需要对这个簇进行划分。

二分K均值算法可以加速K-means算法的执行速度，因为它的相似度计算少了并且不受初始化问题的影响，因为这里不存在随机点的选取，且每一步都保证了误差最小。









## 6.4、特征工程-特征降维

**降维**是指在某些限定条件下，**降低随机变量(特征)个数**，得到**一组"不相关"主变量**的过程。例如三维变二维，一般降维的都是特征之间有相关性，比如相对湿度和降雨量两个特征，相对湿度越大降雨量越大，降雨量越大相对湿度越大，则我们将两个特征去掉一个或者两个特征合并为一个。

- 某些特征的取值较为接近，其包含的信息较少
- 希望特征独立存在对预测产生影响，两个特征同增同减非常相关，不会给模型带来更多信息

降维有三种方式：

1. **低方差过滤法**
2. **相关系数(皮尔逊相关系数，斯皮尔曼相关系数)**
3. **主成分分析（pca降维法）**



### 6.4.1、低方差过滤法

低方差过滤法：指的是删除方差低于某些阈值的特征。

- 特征方差小：**特征值的波动范围小**，包含的信息少，模型很难学习到信息
- 特征方差大：**特征值的波动范围大**，包含的信息相对丰富，便于模型进行学习

我们希望保留特征方差大的特征，因为特征方差小的特征可能比较相似，没必要都考虑。

```python
# 删除方差 = 0.0 的特征
sklearn.feature_selection.VarianceThreshold(threshold = 0.0)
```









### 6.4.2、皮尔逊相关系数

皮尔逊相关系数作用：反映变量之间相关关系密切程度的统计指标。

相关系数的值介于–1与+1之间，即–1≤ r ≤+1。其性质如下：

- **当r>0时，表示两变量正相关，r<0时，两变量为负相关**
- 当|r|=1时，表示两变量为完全相关，当r=0时，表示两变量间无相关关系
- **当0<|r|<1时，表示两变量存在一定程度的相关。且|r|越接近1，两变量间线性关系越密切；|r|越接近于0，表示两变量的线性相关越弱**
- **一般可按三级划分：|r|<0.4为低度相关；0.4≤|r|<0.7为显著性相关；0.7≤|r|<1为高度线性相关**

```python
from scipy.stats import pearsonr

x1 = [12.5, 15.3, 23.2, 26.4, 33.5, 34.4, 39.4, 45.2, 55.4, 60.9]
x2 = [21.2, 23.9, 32.9, 34.1, 42.5, 43.2, 49.0, 52.8, 59.4, 63.5]

pearsonr(x1, x2)
```

![](黑马人工智能(三).assets/27.png)

- **皮尔逊**相关**的结果**包括两个值，相关系数和P值。 
- 相关系数在[-1，1]之间， p值越小，表示相关系数越显著

### 6.4.3、斯皮尔曼相关系数

斯皮尔曼相关系数也是反映变量之间相关关系密切程度的统计指标。与之前的皮尔逊相关系数大小性质一样，取值 [-1, 1]之间，斯皮尔曼相关系数比皮尔逊相关系数应用更加广泛。

```python
from scipy.stats import spearmanr

x1 = [12.5, 15.3, 23.2, 26.4, 33.5, 34.4, 39.4, 45.2, 55.4, 60.9]
x2 = [21.2, 23.9, 32.9, 34.1, 42.5, 43.2, 49.0, 52.8, 59.4, 63.5]

print(spearmanr(x1, x2))
```

![](黑马人工智能(三).assets/28.png)





### 6.4.4、主成分分析(pca降维)

- 定义：**高维数据转化为低维数据的过程**，在此过程中**可能会舍弃原有数据、创造新的变量**
- 作用：**是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。**
- 应用：回归分析或者聚类分析当中

![](黑马人工智能(三).assets/29.png)

例如，我们通过一维的左视图、右视图、俯视图等，最终得到立体的茶壶。虽然损失了少量信息，但是茶壶的大量数据依然可以被获取到。



```python
# n_components: 小数表示保留百分之多少的信息；整数表示减少到多少特征
sklearn.decomposition.PCA(n_components=None)
```



# 7、算法指导

关于在计算的过程中，如何选择合适的算法进行计算，可以参考scikit learn官方给的指导意见：

![](黑马人工智能(三).assets/30.png)













# 9、支持向量机SVM

来玩一个游戏，要求你用一根棍分开两种颜色的球：

![](黑马人工智能(三).assets/33.png)

1. 最开始你直接分开，Very Good！
2. 我将球的位置变动，发现你之前的就不适用了？咋办？把小棍子变粗！
3. **支持向量机SVM就是试图把棍子放在最佳位置，好让在棍的两边有尽可能大的间隙**。

![](黑马人工智能(三).assets/34.png)

4. 我让球分布不均，你选择拍桌子，直接把球震飞，拿一张纸插到两种求中间，这样看球也像是被一条曲线分开了。

---

- 球: 数据 data
- 棍子:分类 classifier
- 最大间隙: 最优化 optimization 
- 拍桌子: 核方法 kernelling
- 纸: 超平面 hyperplane



## 9.1、定义

SVM(Supportted vector machine)支持向量机，就是**寻找到一个超平面使样本分为两类，并且间隙最大**。SVM能够执行**线性或非线性分类、回归，甚至是异常值检测任务**。它是机器学习领域最受欢迎的模型之一。SVM特别适用于中小型复杂数据集的分类。

![](黑马人工智能(三).assets/35.png)





- 紧邻超平面的样本点叫做**支持向量**.



## 9.2、硬间隔和软间隔

在上面我们使用超平面进行分割数据的过程中，如果我们**严格地让所有实例都不在最大间隔之间**，并且位于正确的一边，这就是硬间隔分类。
硬间隔分类有两个问题，

1. 首先，它只在**数据是线性可分离的时候才有效**；
2. 其次，**它对异常值非常敏感**
3. 如果出现异常值、或者样本不能线性可分，此时硬间隔无法实现

要避免这些问题，最好使用更灵活的模型。**目标是尽可能在保持最大间隔宽阔和限制间隔违例（即位于最大间隔之上，甚至在错误的一边的实例)之间找到良好的平衡**，这就是软间隔分类。

- 目标是尽可能在保持间隔宽阔和限制间隔违例之间找到良好的平衡。SKlearn中通过惩罚系数C来控制这个平衡：C值越小，则间隔越宽，但是间隔违例也会越多。



## 9.3、核函数(拍桌子)

核函数：当数据线性不可分时，将原始数据**映射**到高维空间，使得样本能够线性可分

![](黑马人工智能(三).assets/36.png)





## 9.4、原理

SVM要去求一组参数(w,b)，使其构建的超平面函数能够最优的分离两个集合。

![](黑马人工智能(三).assets/37.png)





- 超平面：wx+b=0

- 支持向量：wx+b=1、wx+b=-1
- 超平面的宽度为: 2/w
- 图中 + 的样本点带入 wx+b≥1 ，图中 - 的样本点带入 wx+b≤1 

我们希望找到这么一个超平面，w足够小，同时又能够把两类点完美分割开来。



































































